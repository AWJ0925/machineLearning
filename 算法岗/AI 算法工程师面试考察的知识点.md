# 算法工程师（AI、图像）面试考察的知识点

## 1. **数据结构和算法**

除了写代码实现，还应该掌握如何计算**时间**和**空间**复杂度。

举例：

（1）会写常见的算法的实现？比如，排序等等

```python
def bubbleSort(arr):
    #大的往后放
    n = len(arr)
 
    # 遍历所有数组元素
    for i in range(n):
        # Last i elements are already in place
        for j in range(n-i-1):
            if arr[j] > arr[j+1] :
                arr[j], arr[j+1] = arr[j+1], arr[j]
```

（2）TopK问题--100W个数中找出其中最大的前K个数?

![Top-k](C:\Users\49362\Desktop\算法岗\Top-k.png)

（3）**快排**的时间复杂度？

​	快排在最糟糕得情况下时间复杂度是`O(n²)`，平均的复杂度是`O(nlogn)`，这里的n和logn, 分别代表了调用栈的高度，和完成每层的时间，在a取数组的第一项时候，是最糟的情况，完成每层需要的时间是n，栈的高度是n，时间复杂度就是n²，当取中间的值得时候，完成每层的时间是n，但是调用栈的高度变成了logn，所以这个时候时间复杂度是nlogn。**选择排序**的时间复杂度是O(n²)。

（4）**数据结构**分为几类？（集合、线性结构、树状结构、图）

## 2. **编程语言**

**Python** 为主，部分岗位会有 C++，Java 相对少一些。

举例：

### 2.1 **python**中：

-  **list**、**dict**、**set** 分别对应哪个基本数据结构；插入、删除、查找的时间复杂度分别是多少？
- Python 中**装饰器**(decorator)的作用？

### 2.2 **C++**中：

- （1）**引用**和**指针**的异同？

- （2）**堆内存**和**栈内存**的区别？

  - 数据结构中的**堆**与**栈**：

  `栈`：是一种**连续**储存的数据结构，具有**先进后出**的性质。通常的操作有入栈（圧栈）、出栈和栈顶元素。想要读取栈中的某个元素，就要将其之前的所有元素出栈才能完成。类比现实中的箱子一样。

  `堆`：是一种**非连续**的树形储存数据结构，每个节点有一个值，整棵树是**经过排序**的。特点是根结点的值最小（或最大），且根结点的两个子树也是一个堆。常用来实现优先队列，存取随意。

  <img src="C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220109212337485.png" alt="image-20220109212337485" style="zoom:80%;" />

  **栈内存**：由程序**自动**向操作系统申请分配以及回收，速度快，使用方便，但程序员无法控制。若分配失败，则提示栈溢出错误。注意，const局部变量也储存在栈区内，栈区向地址减小的方向增长。

  **堆内存**：程序员向操作系统**申请**一块内存。当系统收到程序的申请时，会遍历一个记录空闲内存地址的链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序。分配的速度较慢，地址不连续，容易碎片化。此外，由程序员申请，同时也必须由程序员负责销毁，否则会导致**内存泄露**。

-  （3）**new**和**malloc**的区别？

  ```c++
  2.1 属性。new和delete是C++关键字，需要编译器支持；malloc和free是库函数，需要头文件支持。
  2.2 参数。使用new操作符申请内存分配时无须指定内存块的大小，编译器会根据类型信息自行计算。而malloc则需要显式地指出所需内存的尺寸。
  2.3 返回类型。new操作符内存分配成功时，返回的是对象类型的指针，类型严格与对象匹配，无须进行类型转换，故new是符合类型安全性的操作符。而malloc内存分配成功则是返回void * ，需要通过强制类型转换将void*指针转换成我们需要的类型。
  2.4 自定义类型。 new会先调用operator new函数，申请足够的内存（通常底层使用malloc实现）。然后调用类型的构造函数，初始化成员变量，最后返回自定义类型指针。delete先调用析构函数，然后调用operator delete函数释放内存（通常底层使用free实现）。 malloc/free是库函数，只能动态的申请和释放内存，无法强制要求其做自定义类型对象构造和析构工作。
  2.5 “重载”。C++允许自定义operator new 和 operator delete 函数控制动态内存的分配。
  2.6 内存区域。new做两件事：分配内存和调用类的构造函数，delete是：调用类的析构函数和释放内存。而malloc和free只是分配和释放内存。new操作符从自由存储区（free store）上为对象动态分配内存空间，而malloc函数从堆上动态分配内存。自由存储区是C++基于new操作符的一个抽象概念，凡是通过new操作符进行内存申请，该内存即为自由存储区。而堆是操作系统中的术语，是操作系统所维护的一块特殊内存，用于程序的内存动态分配，C语言使用malloc从堆上分配内存，使用free释放已分配的对应内存。自由存储区不等于堆，如上所述，布局new就可以不位于堆中。
  2.7 分配失败。new内存分配失败时，会抛出bac_alloc异常。malloc分配内存失败时返回NULL。
  2.8 内存泄漏。内存泄漏对于new和malloc都能检测出来，而new可以指明是哪个文件的哪一行，malloc确不可以。
  ```
  
- 什么是**虚函数**，**虚函数**的作用？

  为了满足`多态`与`泛型编程`这一性质，C++允许用户使用虚函数 **(virtual function)** 来完成 **运行时决议** 这一操作，这与一般的 **编译时决定** 有着本质的区别。

  虚函数的实现是由两个部分组成的，虚函数**指针**与虚函数**表**。

  

- 

## 3. **机器学习**/**深度学习**基础知识

举例：

- （1）什么是**过拟合**，如何判断模型出现了过拟合；解决过拟合的常用方法？

  ```html
  过拟合是指学习时选择的模型所包含的参数过多，以至于出现这一模型对已知数据预测的很好，但对未知数据预测的很差的现象，可以说【模型选择】旨在避免过拟合并提高模型的预测能力。
  ```

  ![image-20220109162915291](C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220109162915291.png)

  **避免过拟合的方法**：

  ```
  （1）Early stopping：
  （2）数据集扩增：
  	数据集扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。
  （3）正则化方法：
  （4）Dropout：
  	Dropout方法是通过修改ANN中隐藏层的神经元个数来防止ANN的过拟合。
  	前向：在训练开始时，随机地删除一些（可以设定为一半，也可以为1/3，1/4等）【隐藏层】神经元，即认为这些神经元不存在，同时保持输入层与输出层神经元的个数不变。
  	反向：然后按照BP学习算法对ANN中的参数进行学习更新（虚线连接的单元不更新，因为认为这些神经元被临时删除了）。这样一次迭代更新便完成了。下一次迭代中，同样随机删除一些神经元，与上次不一样，做随机选择。这样一直进行瑕疵，直至训练结束。
  ```

- （2）什么是**正则化**，正则化的作用；常见的正则化方法？

  **结构风险最小化**等价于正则化(regularization)，是一种防止过拟合的策略。`结构风险`在`经验风险`上加上表示模型复杂度的`正则化项`。

  正则化的**作用**是选择经验风险与模型复杂度同时较小的模型。

  正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个**正则项**，一般有`L1`正则与`L2`正则等。

- （3）一个完整的经典深度学习过程？

  ```python
  1.数据的预处理
  2.模型的选择
  3.定义评价指标（损失函数、成本函数）
  4.选取算法，优化模型
  5.参数的随机初始化
  ```

- （4）(CV方向) 画一个经典的 CNN，求每层的**感受野**？

- （5）(CV方向) 普通卷积、**分组卷积**、**深度可分离卷积**的关系，计算量比较，使用分组卷积和深度可分离卷积的经典网络？

- （6）**卷积神经网络**：卷积输入输出的大小（特征图）：

  ![image-20220109155830144](C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220109155830144.png)

  

  **注：**区分不同的框架中channels的位置

  ```python
  pytorch中：[batch_size, channels, width, height]
  tensorflow中：[batch_size, width, height, channels]
  ```

- （7）网络的**前向传递**和**反向传递**过程（手推）？

- （8）常见的**优化算法**？

  常见的最优化方法有**梯度下降法**、**牛顿法**和**拟牛顿法**、**共轭梯度法**等等。

  ```
  1. 梯度下降法（Gradient Descent）
  	1）批量梯度下降法（Batch Gradient Descent，BGD）
  	2）随机梯度下降（Random Gradient Descent，RGD）
  ```

- （9）常见的**深度神经网络模型**？

  CNN、RNN、LSTM、Bi-LSTM、ResNet、K-means、决策树

  

- （10）**梯度消失**和**爆炸**怎么解决？

  梯度消失和爆炸的根源是深度网络和反向传播算法。两种情况下`梯度消失`经常出现，一是在**深层网络**中，二是采用了**不合适的损失函数**，比如sigmoid。`梯度爆炸`一般出现在深层网络和**权值初始化值太大**的情况下。

  ![image-20220111142123029](C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220111142123029.png)

  <img src="C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220111142253363.png" alt="image-20220111142253363" style="zoom:50%;" />

  

  <img src="C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220111142346114.png" alt="image-20220111142346114" style="zoom:50%;" />

  用于解决梯度消失和梯度爆炸的**方法**：

  （1）**使用 ReLU、LReLU、ELU、maxout 等激活函数** 。 sigmoid函数的梯度随着x的增大或减小和消失，而ReLU不会。

  （2）**使用批规范化** 。 通过规范化操作将输出信号*x*规范化到均值为0，方差为1，保证网络的稳定性。从上述分析可以看到，反向传播式子中有`w`的存在，所以w的大小影响了梯度的消失和爆炸，Batch Normalization 就是通过对每一层的输出规范为均值和方差一致的方法，消除了*w*带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。 

  **梯度消失、爆炸的解决方案**：

  <img src="C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220111142929163.png" alt="image-20220111142929163" style="zoom:33%;" />

  ![image-20220111143213288](C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220111143213288.png)

  **残差网路：**

  <img src="C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220111143923254.png" alt="image-20220111143923254" style="zoom:50%;" />

  ```diff
  - 预训练加微调
  	其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）
  - 梯度剪切、权重正则（针对梯度爆炸）
  	其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。
  - 使用不同的激活函数
  
  - 使用batchnorm(BN)
  - 使用残差结构
  - 使用LSTM网络
  	长短期记忆网络（long-short term memory networks）。LSTM通过它内部的“门”可以接下来更新的时候“记住”前几次训练的”残留记忆“，因此，经常用于生成文本中
  ```

- （11）神经网络不收敛怎么办？

  理论上，只要训练样本足够多，神经网络可以拟合原始数据分布。

  **原因**：

  - 没有对数据进行归一化
  - 忘记检查输入和输出
  - 没有对数据进行预处理
  - 没有对数据正则化
  - 使用过大的样本
  - 使用不正确的学习率
  - 在输出层使用错误的激活函数
  - 网络中包含坏梯度
  - 初始化权重错误
  - 过深的网络
  - 隐藏单元数量错误

- （12）**降维**有哪些方法?

  ![DR](C:\Users\49362\Desktop\算法岗\DR.jpg)

  降维方法分为**线性**和**非线性**降维，非线性降维又分为基于**核**函数和基于**特征值**的方法

  `线性`降维方法：PCA、ICA、LDA、LFA、LPP(LE的线性表示)

  基于`核函数`的非线性降维方法：KPCA、KICA、KDA 

  基于`特征值`的非线性降维方法（流型学习）：ISOMAP、LLE 、LE、LPP、LTSA、MVU

  ![DR2](C:\Users\49362\Desktop\算法岗\DR2.png)

- （13）为什么是`cove2d`?

  `cove1d`：用于文本数据，只对宽度进行卷积，对高度不进行卷积
   `cove2d`：用于图像数据，对宽度和高度都进行卷积

- （14）**卷积层**和**池化层**的作用？

  <img src="C:\Users\49362\AppData\Roaming\Typora\typora-user-images\image-20220110203158073.png" alt="image-20220110203158073" style="zoom:50%;" />
  
  **卷积层**的主要功能是从输入中提取`特征映射`，由若干个小的矩形矩阵实现，即`卷积核`或滤波器。
  
  **pooling layer**有`局部不变性`而且可以提取显著特征的同时`降低模型的参数`，从而降低模型的过拟合。因为只是提取了显著特征，而舍弃了不显著的信息，使得模型的参数减少了，从而一定程度上可以缓解过拟合的产生。
  
-  (15)

- (16)

- (17)

  

## 4. 业务相关论文

举例：

- (CV方向) Faster R-CNN 的结构，正负样本，`损失函数`，`训练方法`？
- (CV方向) Faster R-CNN 相比之前论文的改进和不足，后续论文如何针对这些不足进行改进？

## 5. 图像处理的基础知识

- （1）图像的过滤器：均值过滤？
- （2）OpenCV 图像库

## 6. 数学题

举例：

- 圆上任取三点，求三点组成锐角三角形的概率。
- 5 条赛道，25 匹马，想找出最快的三匹马，最少需要跑几次。可以认为马的速度为定值，且不存在并列的情况。
- 8个球有一个重一点，最少称几次能找出来